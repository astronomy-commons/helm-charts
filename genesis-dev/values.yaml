rbac:
  enabled: true

nfs-server-provisioner:
  persistence:
    enabled: true
    storageClass: "gp2"
    size: 100Gi

aws-hub:
  jupyterhub:
    hub:
      allowNamedServers: true
      extraConfig:
        run_user_as_root: |
          c.KubeSpawner.uid = 0
          c.Spawner.args.append("--allow-root")
        
        auth: |
          import oauthenticator
          from tornado import gen
          import os, secrets
          
          class MyGitHubAuth(oauthenticator.github.GitHubOAuthenticator):
            @gen.coroutine
            def authenticate(self, handler, data=None):
              userdict = yield super().authenticate(handler, data)
              return userdict

            @gen.coroutine
            def pre_spawn_start(self, user, spawner):
                """Pass upstream_token to spawner via environment variable"""
                auth_state = yield user.get_auth_state()
                if not auth_state:
                    # auth_state not enabled
                    return
                spawner.environment["NB_UID"] = str(auth_state["github_user"]["id"])
                spawner.environment["NB_USER"] = str(auth_state["github_user"]["login"])

          c.JupyterHub.authenticator_class = MyGitHubAuth
          c.Authenticator.enable_auth_state = True
          
          os.environ["JUPYTERHUB_CRYPT_KEY"] = secrets.token_hex(32)

    singleuser:
      # set the hub to deploy an image with AXS installed
      image:
        name: 808034228930.dkr.ecr.us-west-2.amazonaws.com/jupyter-axs
        tag: demo-1567804976

      # assigns the notebook pod the service account called jupyter-spark-serviceaccount in the k8s cluster
      # this service account is created if rbac.enabled is set to true in this Helm chart
      # credentials are mounted in the notebook pod at /var/run/secrets/kubernetes.io/serviceaccount
      # allows Spark (and the user) to access the Kubernetes cluster via the API at https://kubernetes.default.svc:443
      serviceAccountName: jupyter-spark-serviceaccount
      storage:
        # mount the files spark-defaults.conf and spark-env.sh into the user notebook
        # these alter the start-up behavior of Spark
        extraVolumes:
        - name: "spark-config-volume"
          configMap:
            name: "spark-config"
        # consume the nfs PVC that comes with the genesis helm chart
        - name: "nfs-volume"
          persistentVolumeClaim:
            claimName: "nfs"
        # jupyter configurations
        - name: "start-notebook-volume"
          configMap:
            name: "start-notebook.d"
        - name: "before-notebook-volume"
          configMap:
            name: "before-notebook.d"
        - name: "nfs-code-volume"
          persistentVolumeClaim:
            claimName: "nfs-code"
        - name: "s3-scratch-volume"
          persistentVolumeClaim:
            claimName: "csi-s3-pvc"
        # - name: "epyc-sshfs"
        #   persistentVolumeClaim:
        #     claimName: "epyc-sshfs"
        extraVolumeMounts:
        # mount spark configurations from configMap
        # mount spark-defaults.conf to spark-defaults.conf.static
        - name: "spark-config-volume"
          mountPath: "/opt/axs/conf/spark-defaults.conf.static"
          # subPath access the specified file within the config map
          subPath: "spark-defaults.conf"
        # mount spark-env.sh to spark-env.sh.static
        - name: "spark-config-volume"
          mountPath: "/opt/axs/conf/spark-env.sh.static"
          subPath: "spark-env.sh"
        # mount hive-site.xml to hive-site.xml.static
        - name: "spark-config-volume"
          mountPath: "/opt/axs/conf/hive-site.xml.static"
          subPath: "hive-site.xml"
        # mount executor.yaml to executor.yaml.static
        - name: "spark-config-volume"
          mountPath: "/opt/axs/conf/executor.yaml.static"
          subPath: "executor.yaml"
        # mount the efs-backed filesystem to /nfs
        # creates a folder for the user in /nfs and /nfs contains all other users
        - name: "nfs-volume"
          mountPath: "/home/{username}"
          # subPath creates a folder within the efs filesystem for the user
          subPath: "{username}"
        - name: "nfs-volume"
          mountPath: "/home"
        - name: "nfs-code-volume"
          mountPath: "/opt/conda/envs"
        # jupyter configurations
        - name: "start-notebook-volume"
          mountPath: "/usr/local/bin/start-notebook.d"
        - name: "before-notebook-volume"
          mountPath: "/usr/local/bin/before-notebook.d"
        # S3 scratch space
        - name: "s3-scratch-volume"
          mountPath: "/s3"
        # Epyc sshfs
        # - name: "epyc-sshfs"
        #   mountPath: "/epyc"


spark-defaults.conf:
  000-s3-defaults: |
    spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.hadoop.fs.s3a.connection.maximum=10000
  000-kubernetes-defaults: |
    spark.submit.deployMode=client
    spark.master=k8s://https://kubernetes.default.svc:443
  000-scheduler-defaults: |
    # scheduling options (batch size, time out)
    spark.kubernetes.allocation.batch.size=100
    spark.scheduler.maxRegisteredResourcesWaitingTime=600s
    spark.scheduler.minRegisteredResourcesRatio=1.0
  000-executor-defaults: |
    # options for the executor pods
    spark.executor.memory=3500m
    spark.kubernetes.executor.request.cores=0.945
    spark.kubernetes.executor.limit.cores=1.0
  000-sql-defaults: |
    spark.sql.execution.arrow.enabled=true
    spark.sql.hive.metastore.sharedPrefixes=com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc,org.apache.derby
  000-java-defaults: |
    spark.driver.extraJavaOptions -Dderby.system.home=/tmp/derby
  000-jar-defaults: |
    spark.jars /usr/local/axs/python/axs/AxsUtilities-1.0-SNAPSHOT.jar

spark-env.sh:

start-notebook:
  001-env-vars.sh: |
    export SPARK_PUBLIC_DNS="${PUBLIC_URL}${JUPYTERHUB_SERVICE_PREFIX}proxy/4040/jobs/"
    export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
  
  002-spark-defaults.sh: |
    conf_file=$SPARK_HOME/conf/spark-defaults.conf.dynamic
    echo "spark.driver.host=$(hostname -i)" >> $conf_file
    echo "spark.ui.proxyBase=${JUPYTERHUB_SERVICE_PREFIX}proxy/4040" >> $conf_file
    echo "spark.kubernetes.executor.container.image=${JUPYTER_IMAGE}" >> $conf_file
    prefix=spark.executorEnv
    if [ -n "${NB_USER}" ]; then
      echo "${prefix}.NB_USER ${NB_USER}" >> $conf_file
      # echo "spark.kubernetes.executor.podNamePrefix ${NB_USER}-spark" >> $conf_file
      echo "spark.kubernetes.driver.pod.name jupyter-${NB_USER}" | awk '{print tolower($0)}' >> $conf_file
    fi
    if [ -n "${NB_UID}" ]; then
      echo "${prefix}.NB_UID ${NB_UID}" >> $conf_file
    fi
    echo "${prefix}.JAVA_HOME=${JAVA_HOME}" >> $conf_file
  
  999-merge-spark-files.sh: |
    merge () {
      file=$1
      static_file="$file.static"
      dynamic_file="$file.dynamic"

      if [ -f "$file" ]; then
        rm -f $file
      fi
      if [ -f "$static_file" ]; then
        cat $static_file >> $file
      fi
      if [ -f "$dynamic_file" ]; then
        echo "" >> $file
        cat $dynamic_file >> $file
      fi
    }

    merge "$SPARK_HOME/conf/spark-defaults.conf"
    merge "$SPARK_HOME/conf/spark-env.sh"
    merge "$SPARK_HOME/conf/hive-site.xml"
    merge "$SPARK_HOME/conf/executor.yaml"

    # static_conf_file=$SPARK_HOME/conf/spark-defaults.conf.static
    # dynamic_conf_file=$SPARK_HOME/conf/spark-defaults.conf.dynamic
    # conf_file=$SPARK_HOME/conf/spark-defaults.conf

    # if [ -f "$conf_file" ]; then
    #   rm -f $conf_file
    # fi
    # if [ -f "$static_conf_file" ]; then
    #   cat $static_conf_file >> $conf_file
    # fi
    # if [ -f "$dynamic_conf_file" ]; then
    #   echo "" >> $conf_file
    #   cat $dynamic_conf_file >> $conf_file
    # fi

    # static_env_file=$SPARK_HOME/conf/spark-env.sh.static
    # dynamic_env_file=$SPARK_HOME/conf/spark-env.sh.dynamic
    # env_file=$SPARK_HOME/conf/spark-env.sh

    # if [ -f "$env_file" ]; then
    #   rm -f $env_file
    # fi
    # if [ -f "$static_env_file" ]; then
    #   cat $static_env_file >> $env_file
    # fi
    # if [ -f "$dynamic_env_file" ]; then
    #   echo "" >> $env_file
    #   cat $dynamic_env_file >> $env_file
    # fi

    # static_hive_file=$SPARK_HOME/conf/hive-site.xml.static
    # dynamic_hive_file=$SPARK_HOME/conf/hive-site.xml.dynamic
    # hive_file=$SPARK_HOME/conf/hive-site.xml

    # if [ -f "$hive_file" ]; then
    #   rm -f $hive_file
    # fi
    # if [ -f "$static_hive_file" ]; then
    #   cat $static_hive_file >> $hive_file
    # fi
    # if [ -f "$dynamic_hive_file" ]; then
    #   echo "" >> $hive_file
    #   cat $dynamic_hive_file >> $hive_file
    # fi


