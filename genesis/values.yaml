rbac:
  enabled: true

aws-hub:
  jupyterhub:    
    hub:
      allowNamedServers: true
      extraConfig:
        run_user_as_root: |
          c.KubeSpawner.uid = 0
          c.Spawner.args.append("--allow-root")
        
        auth: |
          import oauthenticator
          from tornado import gen
          import os, secrets
          
          class MyGitHubAuth(oauthenticator.github.GitHubOAuthenticator):
            @gen.coroutine
            def authenticate(self, handler, data=None):
              userdict = yield super().authenticate(handler, data)
              return userdict

            @gen.coroutine
            def pre_spawn_start(self, user, spawner):
                """Pass upstream_token to spawner via environment variable"""
                auth_state = yield user.get_auth_state()
                if not auth_state:
                    # auth_state not enabled
                    return
                spawner.environment["NB_UID"] = str(auth_state["github_user"]["id"])
                spawner.environment["NB_USER"] = str(auth_state["github_user"]["login"])

          c.JupyterHub.authenticator_class = MyGitHubAuth
          c.Authenticator.enable_auth_state = True
          
          os.environ["JUPYTERHUB_CRYPT_KEY"] = secrets.token_hex(32)

    singleuser:
      # set the hub to deploy an image with AXS installed
      image:
        name: 808034228930.dkr.ecr.us-west-2.amazonaws.com/jupyter-axs
        tag: demo-1567804976

      # assigns the notebook pod the service account called jupyter-spark-serviceaccount in the k8s cluster
      # this service account is created if rbac.enabled is set to true in this Helm chart
      # credentials are mounted in the notebook pod at /var/run/secrets/kubernetes.io/serviceaccount
      # allows Spark (and the user) to access the Kubernetes cluster via the API at https://kubernetes.default.svc:443
      serviceAccountName: jupyter-spark-serviceaccount
      storage:
        # mount the files spark-defaults.conf and spark-env.sh into the user notebook
        # these alter the start-up behavior of Spark
        extraVolumes:
        - name: "spark-config-volume"
          configMap:
            name: "spark-config"
        # consume the nfs PVC that comes with axs-hub
        - name: "nfs-volume"
          persistentVolumeClaim:
            claimName: "nfs"
        # jupyter configurations
        - name: "start-notebook-volume"
          configMap:
            name: "start-notebook.d"
        - name: "begin-notebook-volume"
          configMap:
            name: "begin-notebook.d"
        extraVolumeMounts:
        # mount spark configurations from configMap
        # mount spark-defaults.conf to spark-defaults.conf.static
        - name: "spark-config-volume"
          mountPath: "/usr/local/axs/conf/spark-defaults.conf.static"
          # subPath access the specified file within the config map
          subPath: "spark-defaults.conf"
        # mount spark-env.sh to spark-env.sh.static
        - name: "spark-config-volume"
          mountPath: "/usr/local/axs/conf/spark-env.sh.static"
          subPath: "spark-env.sh"
        # mount hive-site.xml
        - name: "spark-config-volume"
          mountPath: "/usr/local/axs/conf/hive-site.xml"
          subPath: "hive-site.xml"
        # mount the efs-backed filesystem to /nfs
        # creates a folder for the user in /nfs and /nfs contains all other users
        - name: "nfs-volume"
          mountPath: "/nfs/{username}"
          # subPath creates a folder within the efs filesystem for the user
          subPath: "{username}"
        - name: "nfs-volume"
          mountPath: "/nfs"
        # jupyter configurations
        - name: "start-notebook-volume"
          mountPath: "/usr/local/bin/start-notebook.d"
        - name: "begin-notebook-volume"
          mountPath: "/usr/local/bin/begin-notebook.d"


spark-defaults.conf:
  000-s3-defaults: |
    spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.hadoop.fs.s3a.connection.maximum=10000
  000-kubernetes-defaults: |
    spark.submit.deployMode=client
    spark.master=k8s://https://kubernetes.default.svc:443
  000-scheduler-defaults: |
    # scheduling options (batch size, time out)
    spark.kubernetes.allocation.batch.size=100
    spark.scheduler.maxRegisteredResourcesWaitingTime=600s
    spark.scheduler.minRegisteredResourcesRatio=1.0
  000-executor-defaults: |
    # options for the executor pods
    spark.executor.memory=3500m
    spark.kubernetes.executor.request.cores=0.945
    spark.kubernetes.executor.limit.cores=1.0
  000-sql-defaults: |
    spark.sql.execution.arrow.enabled=true
  000-java-defaults: |
    spark.driver.extraJavaOptions -Dderby.system.home=/tmp/derby
  000-jar-defaults: |
    spark.jars /usr/local/axs/python/axs/AxsUtilities-1.0-SNAPSHOT.jar

spark-env.sh:

hive-site.xml: |
  <?xml version="1.0" encoding="UTF-8" standalone="no"?>
  <configuration>
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:derby:/home/jovyan/.axs/metastore_db;create=true</value>
    </property>
    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>org.apache.derby.jdbc.EmbeddedDriver</value>
    </property>
  </configuration>

start-notebook:
  001-env-vars.sh: |
    export SPARK_PUBLIC_DNS="${PUBLIC_URL}${JUPYTERHUB_SERVICE_PREFIX}proxy/4040/jobs/"
    export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
  
  002-spark-defaults.sh: |
    conf_file=$SPARK_HOME/conf/spark-defaults.conf.dynamic
    echo "spark.driver.host=$(hostname -i)" >> $conf_file
    echo "spark.ui.proxyBase=${JUPYTERHUB_SERVICE_PREFIX}proxy/4040" >> $conf_file
    echo "spark.kubernetes.executor.container.image=${JUPYTER_IMAGE}" >> $conf_file
    prefix=spark.executorEnv
    if [ -n "${NB_USER}" ]; then
      echo "${prefix}.NB_USER ${NB_USER}" >> $conf_file
      echo "spark.kubernetes.executor.podNamePrefix ${NB_USER}-spark" >> $conf_file
      echo "spark.kubernetes.driver.pod.name jupyter-${NB_USER}" | awk '{print tolower($0)}' >> $conf_file
    fi
    if [ -n "${NB_UID}" ]; then
      echo "${prefix}.NB_UID ${NB_UID}" >> $conf_file
    fi
    echo "${prefix}.JAVA_HOME=${JAVA_HOME}" >> $conf_file
  
  999-merge-spark-files.sh: |
    static_conf_file=$SPARK_HOME/conf/spark-defaults.conf.static
    dynamic_conf_file=$SPARK_HOME/conf/spark-defaults.conf.dynamic
    conf_file=$SPARK_HOME/conf/spark-defaults.conf

    if [ -f "$conf_file" ]; then
      rm -f $conf_file
    fi
    if [ -f "$static_conf_file" ]; then
      cat $static_conf_file >> $conf_file
    fi
    if [ -f "$dynamic_conf_file" ]; then
      echo "" >> $conf_file
      cat $dynamic_conf_file >> $conf_file
    fi

    static_env_file=$SPARK_HOME/conf/spark-env.sh.static
    dynamic_env_file=$SPARK_HOME/conf/spark-env.sh.dynamic
    env_file=$SPARK_HOME/conf/spark-env.sh

    if [ -f "$env_file" ]; then
      rm -f $env_file
    fi
    if [ -f "$static_env_file" ]; then
      cat $static_env_file >> $env_file
    fi
    if [ -f "$dynamic_env_file" ]; then
      echo "" >> $env_file
      cat $dynamic_env_file >> $env_file
    fi

